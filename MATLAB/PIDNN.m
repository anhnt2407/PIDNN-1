clear all;close all;%% Neural Net for PID Tuning% ====  INPUTS  ====ts = 0.01;tend = 500;t = 0:ts:tend;length_t = numel(t);%Tolerancesr_tol = 1e-4;%bp_flag = true;%flags = zeros(1, numel(t));%FOR TESTING ONLYx_i = zeros(numel(t), 2);u_h = zeros(numel(t), 3);x_h = zeros(numel(t), 3);v_o = zeros(numel(t), 1);v_change = zeros(1, numel(t));y_change = zeros(1, numel(t));dow = zeros(numel(t), 3);dy_dv = zeros(1, numel(t));% Set Pointsetpoint = 10;% Dynamic System VariablesG = 10;a = 0.1;b = 1;c = 0.1;sys_out = zeros(1, numel(0:ts:tend));s_ow = zeros(numel(sys_out), 3);error = zeros(numel(sys_out), 1);%create setpoint r = sys_out;r = ones(numel(sys_out), 1);r = cos(0.6.*(0:ts:tend));%r = sin(2.*pi.*(0:ts:tend));%r = sin((0:ts:tend));tindex = 0:ts:tend;r = 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-40;r(4001:numel(t)) = 0.5 + 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-80;r(8001:numel(t)) = 1 + 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-120;r(12001:numel(t)) = 1.5 - 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-160;r(16001:numel(t)) = 1 - 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-200;r(20001:numel(t)) = -0.5 + 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-240;r(24001:numel(t)) = 0.5 + 1./(1+exp(-1.*(tindex./2)));tindex = 0:ts:tend-280;r(28001:numel(t)) = 1 + 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-320;r(32001:numel(t)) = 2.5 - 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-360;r(36001:numel(t)) = 1.5 - 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-400;r(40001:numel(t)) = 0 + 1./(1+exp(-1.*tindex));tindex = 0:ts:tend-440;r(44001:numel(t)) = 0.5 + 1./(1+exp(-1.*tindex));% Introducing Noise% noise_Amp = 0.02;% r = r + noise_Amp*randn(1, numel(t));r = setpoint.*r;%create current_value and transfer function matricesy = zeros(1, numel(0:ts:tend));%y_2 = zeros(1, numel(0:ts:tend));%y_3 = zeros(1, numel(0:ts:tend));w_size = 30;%Initialize the matrices for current & last state for NN (row 1 is current, row 2 is past)i_p = zeros(w_size + 1,2);u_p = zeros(w_size + 1,3);x_p = zeros(w_size + 1,3);v = zeros(w_size + 1,1);%initial weights hw = [ -1, -1, -1; 1, 1, 1];%ow = [0.2, 0, 0]; ow = [0.3579, 0.1399, 0.0160];s_ow(1, :) = ow;    %make simulationxdot = 0;xddot = 0;x = 0;%% ====  START SIMULATION  =====%Start at index 2 (Because of array starting at index 1 in OCTAVE)%i = 2;j = 1;for i = 2:numel(t)  %% Input Layer Neurons  j = j;  i_p(j+1,1) = ptransfer(y(i-1));  i_p(j+1,2) = ptransfer(r(i));  %% Hidden Layer Neurons  %1. P Neuron  u_p(j+1,1) = i_p(j+1,1)*hw(1,1) + i_p(j+1,2)*hw(2,1);  x_p(j+1,1) = ptransfer(u_p(j+1));  %2. I Neuron  u_p(j+1,2) = i_p(j+1,1)*hw(1,2) + i_p(j+1,2)*hw(2,2);  x_p(j+1,2) = itransfer(u_p(j+1,2), x_p(j,2), ts);  %3. D Neuron  u_p(j+1,3) = i_p(j+1,1)*hw(1,3) + i_p(j+1,2)*hw(2,3);  x_p(j+1,3) = dtransfer(u_p(j+1,3), u_p(j,3), ts);  %% Output Layer  v(j+1) = ow*(x_p(j+1,:)');  %% Feed the force in the second-order system%   xddot = ((v(j+1)*G)/a) - xdot*b - x*c;%   xdot = xdot + xddot*ts;%   x = x + xdot*ts;  x = x + v(j)*ts;  y(i) = x;    error(i) = y(i) - r(i);  if j == w_size  %display("BACK PROPAGATION");    [ow, dow(i,:), hw] = back_propagation_batch(r(i-w_size:i), y(i-w_size:i), w_size, ow, hw, v, x_p, u_p, i_p, r_tol, ts);        % Set the past values for the next batch    i_p(1,:) = i_p(j+1,:);    u_p(1,:) = u_p(j+1,:);    x_p(1,:) = x_p(j+1,:);    v(1) = v(j+1);        %Reset j    j = 0;  end  j = j + 1;   %  ====== For Testing Purposes Only ======     % Tolerance tol = 1e-20;  % Change in measured position if i == 1   dy = y(i); else   dy = y(i) - y(i-1); end y_change(i) = dy;    % Change in Neural Net Output dv = v(1) - v(2); if abs(dv) < tol  dv = sign(dv)*tol; end v_change(i) = dv;  dy_dv(i) = dy/dv;   x_i(i,:) = i_p(1,:); u_h(i,:) = u_p(1,:); x_h(i,:) = x_p(1,:); v_o(i) = v(1);   %=======================================    s_ow(i, :) = ow;    i = i + 1;    end%h1 = figure(1);plot(1:length_t, y, 'color', 'red');hold on;plot(1:length_t, r, 'color', 'green');%plot(0:ts:tend, r, 'linewidth', 2);h2 = figure(2)subplot(3,1,1);plot(1:length_t, s_ow(:,1));xlabel('Time(sec)');ylabel('P Output Weight');title('Proportional Output Weight');hold on;subplot(3,1,2);plot(1:length_t, s_ow(:,2));xlabel('Time(sec)');ylabel('I Output Weight');title('Integral Output Weight');hold on;subplot(3,1,3);plot(1:length_t, s_ow(:,3));xlabel('Time(sec)');ylabel('D Output Weight');title('Derivative Output Weight');%hold on;%subplot(3,3,8);%plot(230:ts:tend, flags(2301:length_t));%xlabel("Time(sec)");%ylabel("Back Propagation");%title("Back Propagation Boolean");h3 = figure(3);%%% Changes for DR, DY, and DVplot(1:length_t, error);xlabel('Time(sec)');ylabel('DR');title('Error With Respect To Time');h4 = figure(4)'plot(1:length_t, v_change);xlabel("Time(sec)");ylabel("DV");title("Difference in Neural Net Output");